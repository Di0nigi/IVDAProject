\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage[numbers]{natbib} 
\usepackage{float} 
\usepackage{ai-usage-card}
% Margins
\usepackage[top=1cm, left=2cm, right=2cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}



% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Group Assignment 1: Project Characterization \\ \small Interactive-Visual Data Analysis, Fall 2024}
\author{
\textbf{Dionigi Rodriguez (24-755-688)} \and
\textbf{Cyril Smetanka (24-754-434)} \and
\textbf{Patrick Sproll (19-733-104)}
}
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%
%     Content     %
%%%%%%%%%%%%%%%%%
\section*{Domain}


We have set out to make an interactive surface for Greta Franzini's manually collected ``Catalogue of Digital Editions'', which consists of 350 works from digital humanities projects. Our aim is to enable analysis of these works in relation to each other and to provide an interface that will primarily be used for search actions, mainly exploration, while also providing features for consumption actions like discovery and enjoy. \\
The catalogue has a website already (\href{Link}{https://dig-ed-cat.acdh.oeaw.ac.at/editions.html}), but it is very barebones when it comes to visual exploration, providing only a map of the funding institutions, which doesn't enable exploration in the way that domain experts would ideally navigate this data, as it is missing any grouping of eras, languages, or types of audiences the works are catered towards.

\section*{What}
The catalogue consists of a list of 350
works from digital humanities projects with a total of 52 attributes each (including an ID per item), obtained in tabular form as a CSV file from the following GitHub repository (\href{https://github.com/dig-Eds-cat/digEds_cat}{Link}) \\
Of the 52 attributes, 25 are encoded as dummy variables, 15 as text,
8 as categorical variables, and 4 as numerical variables., of which 3 are dates/years.
Categorical variables allow the editions to be more finely described, again by inherent attribute
e.g. the historical period.
Works range from the antiquity to contemporary, with ``Middle Ages'', `` Long Nineteenth Century'' and `` Early Modern'' dominating. Predominantly, editions are
in either English, Latin or German, which comprise more than half of the catalogue.\\
Of the 18'252 cells, only 8 have missing values, in just three columns.
Given the data was prepared by an experienced researcher,
we consider it both reliable and clean, requiring little preprocessing except the handling of the missing values, which will be done through replacement.
We intend to augment the dataset with a number of LLM generated attributes to facilitate exploration and assessment of works by researchers through similarities and connections between items:
Author school of thought, 5 keywords, authoritativeness, and a statement for the renown of a work as well as a
quick work description to provide some detail on the contents of a work during exploration. We intend to use mainly intrinsic attributes for the project, not the technical ones, as the target audience is
primarily interested in these aspects.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Dummy_variable_dist.png}
    \caption{Distribution of dummy variables in the catalogue.}
    \label{fig:dummy_variable_dist}
\end{figure}

\section*{Why}
The goal according to the problem statement is to let users (primarily historians, researchers and digital library
users) explore the contents of the catalogue, while also allowing them to do a directed search. The task falls squarely into the data exploration and interactive relation
discovery research streams. We are considering a sub tool that enables users to tag their own connections
between works, which would touch upon interactive data labeling. The following tasks need to be
covered:
%actual tasks selected
\begin{itemize}
    \item Gain overview -- Users will need to get a grasp of what they are able to find in the catalogue and if there may
be something that fits their needs and interests, focused on intrinsic attributes, not technical ones.
    \item Identify relationships -- We facilitate this by guiding exploration among relationships between works,
such as time period, or author. We will use clustering and LLM generated keywords to find similar works.
    \item Filter by attributes -- To reduce noise and narrow down the search space, users need to be able to
filter by all attributes, technical and non technical.
    \item Judge reliability -- Users need to be able to assess the quality of the data. We will provide LLM generated
attributes such as renown and authoritativeness along side the values contained in the catalogue. We will
provide an overall score as well.
    \item Tag relationships -- To record their own train of thought and help organize their research, users need
to be able to tag relationships between works.
          works.
\end{itemize}

\section*{How}
%1 
\textbf{See our visualizations online:} \href{https://www.figma.com/design/r1m9NyPqVG9rJeMMBKWMyT/App-Visualisations?node-id=1-4706&t=drxc6jhEoEmUlro3-1}{Figma} \\

\begin{itemize}
    \item Gain overview: Timeline with
          colored background blocks and works plotted, a block of title, author and
          description of a work, a bar chart of languages, and a cluster plot based on
          keywords as initial overview, all linked and can be narrowed down
          to suit any subset of works.
    \item Identify relationships: A network graph and cluster plot with redundant encoding by colour and shape.
    \item Filter by attributes: A search function by title or author for
          specific queries, as well as traditional filter by attribute. These will come
          in the form of sliders, or drag and drop pills. Where possible, we will also provide
          histograms.
    \item Judge reliability: We will provide a score represented
          as a colored bar, as well as the attributes contributing to the score.
    \item Tag relationships: A network graph where users can add edges
          between works to represent their own relationships and tag them as well as
          organize in groups.
\end{itemize}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{docs/relation_graph.png}
    \caption{Relation graph}
    \label{fig:relation score}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{docs/list.png}
    \caption{List of works}
    \label{fig:list of works}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{docs/reliability_score_settings.png}
    \caption{Reliability score settings}
    \label{fig:reliability score}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{docs/timeline.png}
    \caption{Timeline of works}
    \label{fig:timeline}
\end{figure}

%2
We will provide this in a single dashboard with linked visualisations as outlined above. We will allow users to define global filters and apply them to individual elements.

%4 ML: 
We will use two main techniques, the k-means \cite{8ddb7f85-9a8c-3829-b04e-0476a67eb0fd} algorithm and PCA \cite{DBLP:journals/corr/Shlens14} (Principal Component Analysis), to identify significant clusters and represent the data in a lower-dimensional space. 
In addition, we will leverage a pretrained transformer model (e.g., BERT \cite{DBLP:journals/corr/abs-1810-04805}) to generate semantically meaningful embeddings\cite{thePowerOfEmbeddings} of various attributes, providing a numerical representation of each article for use with the clustering and dimensionality reduction algorithms. 

% dont forget: max 250 words per section, no more than 4 pages of text (excl any visualizations and references)

\section*{Group Dynamics}
Given that our data is fairly clean and well structured already, we have no need for an explicit Data Steward/ Data Shaper role as outlined in the course material.
Dionigi will largely take on the roles of data engineer and ML/AI engineer, supported by Cyril, as both study AI. Cyril will also take on the
role of a generalist, supporting frontend development and documentation. While we are lacking a research scientist, Patrick will take on the
role of technical analyst, and support Cyril in frontend development and documentation as well. All together will serve as evangelists in so far as it is necessary.

%%%%%%%%%%%%%%%%%
\newpage
%     Example reference in IEEE style:     %
\bibliographystyle{IEEEtranN}
\bibliography{references}
\newpage
\makeAIUsageCard
%%%%%%%%%%%%%%%%%
\end{document}
